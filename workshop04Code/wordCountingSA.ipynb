{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## COSC2671 Social Media and Network Analytics\n",
    "\n",
    "## Workshop 4\n",
    "\n",
    "Jeffrey Chan, RMIT University, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing packages and nltk data libraries\n",
    "import string\n",
    "import json\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# load the twitter processing python class for use\n",
    "# for those code that we repeatingly use but doesn't change much, or only change due to input, it is good to write them as\n",
    "# functions and call later, which we are doing here.\n",
    "# As the weeks goes by, we will increasingly do and make use of the benefits of scripts but also the interactivity of\n",
    "# interactive Jupyter notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# we are importing TwitterProcessing.py and its contents\n",
    "%aimport TwitterProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "countWordSentimentAnalysis(): First approach for unsupervised sentiment analysis.\n",
    "\n",
    "Complete the parts with TODO for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def countWords(tweetText, wordsToCount):\n",
    "    print(tweetText)\n",
    "\n",
    "    wordCount = 0\n",
    "    for word in tweetText:\n",
    "        if word in wordsToCount:\n",
    "            wordCount += 1\n",
    "    return wordCount\n",
    "\n",
    "\n",
    "def countWordSentimentAnalysis(setPosWords, setNegWords, sTweetsFilename, bPrint, tweetProcessor):\n",
    "    \"\"\"\n",
    "    Basic sentiment analysis.  Count the number of positive words, count the negative words, overall polarity is the\n",
    "    difference in the two numbers.\n",
    "\n",
    "    @param setPosWords: set of positive sentiment words\n",
    "    @param setNegWords: set of negative sentiment words\n",
    "    @param sTweetsFilename: name of input file containing a json formated tweet dump\n",
    "    @param bPrint: whether to print the stream of tokens and sentiment.  Uses colorama to highlight sentiment words.\n",
    "    @param tweetProcessor: TweetProcessing object, used to pre-process each tweet.\n",
    "\n",
    "    @returns: list of tweets, in the format of [date, sentiment]\n",
    "    \"\"\"\n",
    "\n",
    "    lSentiment = []\n",
    "    # open file and process tweets, one by one\n",
    "    with open(sTweetsFilename, 'r') as f:\n",
    "        \n",
    "        # Note that this is using the older data format, but same ideas still apply\n",
    "        for line in f:\n",
    "            # each line is loaded according to json format, into tweet, which is actually a dictionary\n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            try:\n",
    "                tweetText = tweet.get('text', '')\n",
    "                tweetDate = tweet.get('created_at')\n",
    "                # pre-process the tweet text\n",
    "                lTokens = tweetProcessor.process(tweetText)\n",
    "\n",
    "                # Compute sentiment value\n",
    "                numberPosWords = countWords(lTokens, setPosWords)\n",
    "                numberNegWords = countWords(lTokens, setNegWords)\n",
    "                sentiment =  numberPosWords - numberNegWords\n",
    "\n",
    "                # save the date and sentiment of each tweet (used for time series)\n",
    "                lSentiment.append([pd.to_datetime(tweetDate), sentiment])\n",
    "\n",
    "                # if we are printing, each token is printed and coloured according to red if positive word, and blue\n",
    "                # if negative\n",
    "                if bPrint:\n",
    "                    for token in lTokens:\n",
    "                        if token in setPosWords:\n",
    "                            print(Fore.RED + token + ', ', end='')\n",
    "                        elif token in setNegWords:\n",
    "                            print(Fore.BLUE + token + ', ', end='')\n",
    "                        else:\n",
    "                            print(Style.RESET_ALL + token + ', ', end='')\n",
    "\n",
    "                    print(': {}'.format(sentiment))\n",
    "\n",
    "\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "    return lSentiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "vaderSentimentAnalysis(): Second approach for unsupervised sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vaderSentimentAnalysis(sTweetsFilename, bPrint, tweetProcessor):\n",
    "    \"\"\"\n",
    "    Use Vader lexicons instead of a raw positive and negative word count.\n",
    "\n",
    "    @param sTweetsFilename: name of input file containing a json formated tweet dump\n",
    "    @param bPrint: whether to print the stream of tokens and sentiment.  Uses colorama to highlight sentiment words.\n",
    "    @param tweetProcessor: TweetProcessing object, used to pre-process each tweet.\n",
    "\n",
    "    @returns: list of tweets, in the format of [date, sentiment]\n",
    "    \"\"\"\n",
    "\n",
    "    # this is the vader sentiment analyser, part of nltk\n",
    "    sentAnalyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "    lSentiment = []\n",
    "    # open file and process tweets, one by one\n",
    "    with open(sTweetsFilename, 'r') as f:\n",
    "        for line in f:\n",
    "            # each line is loaded according to json format, into tweet, which is actually a dictionary\n",
    "            tweet = json.loads(line)\n",
    "\n",
    "            try:\n",
    "                tweetText = tweet.get('text', '')\n",
    "                tweetDate = tweet.get('created_at')\n",
    "                # pre-process the tweet text\n",
    "#                 lTokens = tweetProcessor.process(tweetText)\n",
    "\n",
    "                # this computes the sentiment scores (called polarity score in nltk, but mean same thing essentially)\n",
    "                # see lab sheet for what dSentimentScores holds\n",
    "#                 dSentimentScores = sentAnalyser.polarity_scores(\" \".join(lTokens))\n",
    "                dSentimentScores = sentAnalyser.polarity_scores(tweetText)\n",
    "\n",
    "\n",
    "                # save the date and sentiment of each tweet (used for time series)\n",
    "                lSentiment.append([pd.to_datetime(tweetDate), dSentimentScores['compound']])\n",
    "\n",
    "                # if we are printing, we print the tokens then the sentiment scores.  Because we don't have the list\n",
    "                # of positive and negative words, we cannot use colorama to label each token\n",
    "                if bPrint:\n",
    "#                     print(*lTokens, sep=', ')\n",
    "                    # uncomment this (and comment above line) if you are analysing the tweets directly, instead of the tokens\n",
    "                    print(tweetText)\n",
    "                    for cat,score in dSentimentScores.items():\n",
    "                        print('{0}: {1}, '.format(cat, score), end='')\n",
    "                    print()\n",
    "\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "\n",
    "\n",
    "    return lSentiment\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following code will run the approaches.\n",
    "\n",
    "First we set the parameters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# arguments for this notebook\n",
    "# modify as needed if you want to do similar analaysis for other purposes\n",
    "\n",
    "# input file of set of postive words\n",
    "posWordFile = 'positive-words.txt'\n",
    "# input file of set of negative words\n",
    "negWordFile = 'negative-words.txt'\n",
    "# input file of set of tweets (json format)\n",
    "tweetsFile = 'rmitCsTwitterTimeline.json'\n",
    "# flag to determine whether to print out tweets and their sentiment\n",
    "flagPrint = True\n",
    "# specify the approach to take, one of [count, vader]\n",
    "# change this to use a different sentiment approach\n",
    "approach = 'count'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then following calls the functions we have written above.  First do the pre-processing, then calls the functions to do the sentiment analysis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the main part of the notebook, that will can and run the various methods defined before.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# construct the tweet pro-processing object\n",
    "tweetTokenizer = TweetTokenizer()\n",
    "lPunct = list(string.punctuation)\n",
    "# standard 'English' stopwords plus we want to remove things like 'rt' (retweet) etc\n",
    "lStopwords = stopwords.words('english') + lPunct + ['rt', 'via', '...', 'â€¦', '\"', \"'\", '`']\n",
    "\n",
    "# call the TwitterProcessing python script\n",
    "tweetProcessor = TwitterProcessing.TwitterProcessing(tweetTokenizer, lStopwords)\n",
    "\n",
    "\n",
    "# load set of positive words\n",
    "lPosWords = []\n",
    "with open(posWordFile, 'r', encoding='utf-8', errors='ignore') as fPos:\n",
    "    for sLine in fPos:\n",
    "        lPosWords.append(sLine.strip())\n",
    "\n",
    "setPosWords = set(lPosWords)\n",
    "\n",
    "\n",
    "# load set of negative words\n",
    "lNegWords = []\n",
    "with codecs.open(negWordFile, 'r', encoding='utf-8', errors='ignore') as fNeg:\n",
    "    for sLine in fNeg:\n",
    "        lNegWords.append(sLine.strip())\n",
    "\n",
    "setNegWords = set(lNegWords)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute the sentiment\n",
    "# to change method, update parameter settings, particularly the variable 'approach' and rerun the parameter setting cell,\n",
    "# and also this cell\n",
    "lSentiment = []\n",
    "if approach == 'count':\n",
    "    lSentiment = countWordSentimentAnalysis(setPosWords, setNegWords, tweetsFile, flagPrint, tweetProcessor)\n",
    "elif approach == 'vader':\n",
    "    lSentiment = vaderSentimentAnalysis(tweetsFile, flagPrint, tweetProcessor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "series = pd.DataFrame(lSentiment, columns=['date', 'sentiment'])\n",
    "series.set_index('date', inplace=True)\n",
    "series[['sentiment']] = series[['sentiment']].apply(pd.to_numeric)\n",
    "\n",
    "series.plot()\n",
    "plt.show()\n",
    "\n",
    "series3 = series.resample('1H').sum()\n",
    "series3.plot()\n",
    "plt.show()\n",
    "\n",
    "newSeries = series.resample('1D').sum()\n",
    "newSeries.plot()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}