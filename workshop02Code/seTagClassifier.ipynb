{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## COSC2671 Social Media and Network Analytics Workshop Week 2\n",
    "\n",
    "Jeffrey Chan, RMIT University 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import a number of needed packages and classes.  When you run this the first time and haven't done so before, nltk may download a list of stopwords, and have output to the tune of \"Downloading package stopwords to ...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from argparse import ArgumentParser\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Parameters for our Notebook.  Please change as needed if your input is named differently or want to change the parameters max_df and min_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputPostFile='filteredPosts.json'\n",
    "max_df=1.0\n",
    "min_df=3"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following command will open the posts, parse out the title and body of question and tags, apply a vectorizer to turn that data into a vector form, then apply a SVM classifier.  Please examine the code carefully, there are comments throughout to help you understand, and additionally look at the worksheet plus ask your friendly demonstrator if unsure about something.  Run the code and examine the output, then refer to the worksheet to see what you need to change."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llLabels:  7033\n",
      "lPosts:  7033\n",
      "Shape of document-word matrix X (posts, words):  (7033, 10234)\n",
      "Shape of y:  (7033, 169)\n",
      "What open-source books (or other materials) provide a relatively thorough overview of data science?As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers. What open-source books (or other materials) provide a relatively thorough overview of data science?As a researcher and instructor, I'm looking for open-source books (or similar materials) that provide a relatively thorough overview of data science from an applied perspective. To be clear, I'm especially interested in a thorough overview that provides material suitable for a college-level course, not particular pieces or papers.\n",
      "['education']\n",
      "  (0, 6694)\t0.11342770160425998\n",
      "  (0, 6894)\t0.16379137177611106\n",
      "  (0, 6740)\t0.10158571767997426\n",
      "  (0, 2618)\t0.10733343749690734\n",
      "  (0, 5449)\t0.10711038328976548\n",
      "  (0, 2229)\t0.1566288875736882\n",
      "  (0, 8830)\t0.11930206763069129\n",
      "  (0, 5748)\t0.15112706552687896\n",
      "  (0, 7285)\t0.14144998125520464\n",
      "  (0, 3589)\t0.1328906021190217\n",
      "  (0, 2133)\t0.11121880815772633\n",
      "  (0, 6855)\t0.1663059718453625\n",
      "  (0, 1244)\t0.112715212084492\n",
      "  (0, 8336)\t0.08230014552345484\n",
      "  (0, 5593)\t0.08168019829274263\n",
      "  (0, 4987)\t0.1968775414356847\n",
      "  (0, 7775)\t0.1740727067133511\n",
      "  (0, 8064)\t0.19234425450941034\n",
      "  (0, 2768)\t0.07917857631737021\n",
      "  (0, 6642)\t0.49137411532833314\n",
      "  (0, 7669)\t0.2590814909882366\n",
      "  (0, 7282)\t0.21466687499381468\n",
      "  (0, 5749)\t0.3413147446684471\n",
      "  (0, 1709)\t0.2713217065419283\n",
      "  (0, 8485)\t0.21863720794532182\n",
      "  (0, 6502)\t0.21511719204018084\n",
      "  (0, 5025)\t0.10185139715570216\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# set of stop words we be using (nltk)\n",
    "stop_list = stopwords.words('english')\n",
    "\n",
    "# list of posts\n",
    "lPosts = []\n",
    "# list of labels, note the indices of this and lPosts should match.\n",
    "llLabels = []\n",
    "\n",
    "# open the input file and parse the posts and tags\n",
    "with open(inputPostFile, 'r') as f:\n",
    "    for sLine in f:\n",
    "        doc = json.loads(sLine)\n",
    "\n",
    "        # parse post has title, then combine the title and body text together\n",
    "        if 'Title' in doc:\n",
    "            post = doc['Title'] + doc['Body']\n",
    "        else:\n",
    "            # parse just parse the body\n",
    "            post = doc['Body']\n",
    "\n",
    "        lPosts.append(post)\n",
    "\n",
    "        # split the string of tags into a list of tags then add to llLabels\n",
    "        llLabels.append(doc['Tags'].split(' '))\n",
    "\n",
    "    print(\"llLabels: \", len(llLabels))\n",
    "    print(\"lPosts: \", len(lPosts))\n",
    "\n",
    "\n",
    "# will do tokenisation, convert to lower cases, remove stop words etc, as well as reweighting using TF-IDF scheme\n",
    "    # and put all of this into a document-word matrix.\n",
    "    # Look at the documentation for more stuff you can do with this class.\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df,\n",
    "                                    stop_words=stop_list,\n",
    "                                    max_df=max_df,\n",
    "                                    lowercase=True)\n",
    "\n",
    "\n",
    "    # Actually build the document-word matrix (X)\n",
    "    X = vectorizer.fit_transform(lPosts)\n",
    "    print(\"Shape of document-word matrix X (posts, words): \", X.shape)\n",
    "\n",
    "    # pca = TruncatedSVD(n_components=1000)\n",
    "    # pca.fit(X)\n",
    "    # newX = pca.transform(X)\n",
    "    # print(\"Shape of newX: \", newX.shape)\n",
    "    # X=newX\n",
    "\n",
    "\n",
    "# scikit learn has another great class, MultiLabelBinarizer, which constructs binary vectors of multilabelled data\n",
    "    # that is, for each class there is a corresding entry in the vector, with a value of '1' if a label is present\n",
    "    # in a document/posts, otherwise '0'\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(llLabels)\n",
    "    print(\"Shape of y: \", y.shape)\n",
    "\n",
    "    print(lPosts[1], lPosts[1])\n",
    "    print(llLabels[1])\n",
    "    print(X[1])\n",
    "    print(y[1])\n",
    "\n",
    "# here we can play with a number of different classifiers\n",
    "    # We have started with a SVM with a linear kernel\n",
    "    classifier = OneVsRestClassifier(KNeighborsClassifier())\n",
    "\n",
    "\n",
    "    lScoringMetric = ['precision_micro', 'recall_micro', 'f1_micro']\n",
    "    lScores = cross_validate(classifier, X, y=y,\n",
    "                             cv=10,\n",
    "                             scoring=lScoringMetric,\n",
    "                             return_train_score=False)\n",
    "\n",
    "\n",
    "    # finally output the average F1 score\n",
    "    print(\"Average precision: \" + str(np.mean(lScores['test_'+lScoringMetric[0]])))\n",
    "    print(\"Average recall: \" + str(np.mean(lScores['test_' + lScoringMetric[1]])))\n",
    "    print(\"Average F1: \" + str(np.mean(lScores['test_' + lScoringMetric[2]])))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}