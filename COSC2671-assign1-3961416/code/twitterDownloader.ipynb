{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "COSC2671 Social Media and Network Analytics\n",
    "\n",
    "# Assignment 1 - Twitter posts downloader\n",
    "\n",
    "@author Lukas Krodinger, s3961415\n",
    "\n",
    "Note that this notebook requires the file twitterClient.py written by Jeffrey Chan with a valid twitter bearerToken, where the limit is not exceeded in order to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from workshop03Code import twitterClient"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def load_tweets(filename):\n",
    "    \"\"\"\n",
    "    Loads the tweets from the file with the given name into an array of tweets.\n",
    "\n",
    "    @param filename: The filename of the file to load the tweets from.\n",
    "\n",
    "    @returns: An array of tweets.\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for sLine in f:\n",
    "            tweet = json.loads(sLine)\n",
    "            tweets.append(tweet)\n",
    "    return tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "client = twitterClient.twitterClient()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, I define the search query, what fields of each tweet to download, the maximum amount of downloaded tweets as well as the name of the output json file.\n",
    "\n",
    "My search focuses on the \"Great Barrier Reef\" and only on english posts.\n",
    "I want to download all tweet fields supported by tweepy and requiring no authentication, as one can still filter out the required fields for analysis later on.\n",
    "Note that the max_tweets might not be reached, because I only download tweets which are at most one-week-old."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define what tweets do download\n",
    "search_query = '\"Great Barrier Reef\" lang:en'\n",
    "\n",
    "# All non-authenticated tweet fields\n",
    "all_tweet_fields = ['id', 'text', 'attachments', 'author_id', 'context_annotations', 'conversation_id', 'created_at', 'entities', 'geo', 'in_reply_to_user_id', 'lang', 'possibly_sensitive', 'public_metrics', 'referenced_tweets', 'reply_settings', 'source', 'withheld']\n",
    "\n",
    "# The maximum amount of tweets to download\n",
    "max_tweets = 100  # 50000 was used here\n",
    "\n",
    "# The filename of the file to store the tweets into\n",
    "all_twitter_fields_filename = \"newTweets.json\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I download the tweets via the tweepy client in a paginated manner (100 at once)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets downloaded:  100\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "# Download the tweets paginated, 100 at once\n",
    "for tweet in tweepy.Paginator(client.search_recent_tweets, search_query, max_results=100, tweet_fields=all_tweet_fields).flatten(limit=max_tweets):\n",
    "    tweets.append(tweet)\n",
    "\n",
    "print(\"Number of tweets downloaded: \", len(tweets))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now I store the downloaded tweets to the specified output file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets successfully stored to:  newDownload.json\n"
     ]
    }
   ],
   "source": [
    "with open(all_twitter_fields_filename, 'w') as json_file:\n",
    "    for tweet in tweets:\n",
    "        json.dump(tweet.data, json_file)\n",
    "        json_file.write('\\n')\n",
    "\n",
    "print(\"Tweets successfully stored to: \", all_twitter_fields_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a next step I first filter all fields of tweets that are of interest. I also only have a look at 5.000 tweets in order to not exceed the file size limit of 5MB, and I also only take tweets into account, that contain the words \"great barrier reef\" in that order."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "tweets = load_tweets(all_twitter_fields_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# The filename of the file to store the filtered tweets\n",
    "filtered_tweet_fields_filename='newFilteredTweets.json'\n",
    "\n",
    "# The fields of interest\n",
    "fields_of_interest = ['id', 'text', 'entities', 'created_at']\n",
    "\n",
    "# What we want our tweets to contain\n",
    "filter_for = \"great barrier reef\"\n",
    "\n",
    "# The amount of tweets we want to filter out\n",
    "amount_of_tweets = 5000\n",
    "\n",
    "# Whether tweets with redundant text should be removed or not\n",
    "remove_redundant_tweets_texts = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I load the posts from the file with all fields and delete all fields I am not interested in. Then I store the tweets with the remaining fields if they contain the filter_for and the amount is not exceeded."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tweets successfully stored to:  filteredTweets.json\n",
      "Filtered tweets successfully stored to:  filteredTweets.json\n"
     ]
    }
   ],
   "source": [
    "with open(all_twitter_fields_filename, 'r') as fIn, open(filtered_tweet_fields_filename, 'w') as fOut:\n",
    "    count = 0\n",
    "    tweet_texts = []\n",
    "\n",
    "    for line in fIn:\n",
    "        tweet = json.loads(line)\n",
    "\n",
    "        # Remove not interesting fields\n",
    "        for key in list(tweet.keys()):\n",
    "            if key not in fields_of_interest:\n",
    "                del tweet[key]\n",
    "\n",
    "        # Remove tweets which do not contain the filter_for text\n",
    "        text = tweet.get(\"text\").lower()\n",
    "        if filter_for not in text:\n",
    "            continue\n",
    "\n",
    "        # Remove redundant text tweets\n",
    "        if remove_redundant_tweets_texts:\n",
    "            if text not in tweet_texts:\n",
    "                tweet_texts.append(text)\n",
    "                count = count + 1\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Take no more than amount_of_tweets\n",
    "        if count > amount_of_tweets:\n",
    "            break\n",
    "\n",
    "        # Store tweet again\n",
    "        fOut.write(\"{}\\n\".format(json.dumps(tweet)))\n",
    "\n",
    "print(\"Filtered tweets successfully stored to: \", filtered_tweet_fields_filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}